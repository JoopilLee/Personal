{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hy1 5점대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.167802917329347\n",
      "5.175871905029951\n",
      "5.155988028267658\n",
      "평균 RMSE: 5.166554283542318\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "path='/home/recordk/reco/BX-Book-Ratings.csv'\n",
    "ratings = pd.read_csv(path)\n",
    "ratings['Book-Rating'] = ratings['Book-Rating'].astype(int)\n",
    "ratings.columns=['user_id','isbn','rating']\n",
    "ratings=ratings[ratings['rating']!=0]\n",
    "ratings=ratings.reset_index(drop=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# LabelEncoder\n",
    "user_encoder = LabelEncoder()\n",
    "isbn_encoder = LabelEncoder()\n",
    "\n",
    "# user_id와 isbn label인코딩\n",
    "ratings['user_id'] = user_encoder.fit_transform(ratings['user_id'])\n",
    "ratings['isbn'] = isbn_encoder.fit_transform(ratings['isbn'])\n",
    "\n",
    "# Dummy recommender 0\n",
    "def recommender0(recomm_list):\n",
    "    recommendations = []\n",
    "    for pair in recomm_list:\n",
    "        recommendations.append(random.random() * 4 + 1)\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Dummy recommender 1\n",
    "def recommender1(recomm_list):\n",
    "    recommendations = []\n",
    "    for pair in recomm_list:\n",
    "        recommendations.append(random.random() * 4 + 1)\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Hybrid 함수\n",
    "def hybrid1(recomm_list, weight=[0.5, 0.5]):\n",
    "    result0 = recommender0(recomm_list)\n",
    "    result1 = recommender1(recomm_list)\n",
    "    result = []\n",
    "    for i, number in enumerate(result0):\n",
    "        result.append(result0[i] * weight[0] + result1[i] * weight[1])\n",
    "    return result\n",
    "\n",
    "# RMSE 계산을 위한 함수\n",
    "def RMSE2(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred)) ** 2))\n",
    "\n",
    "# kfold이용해서 트레인 테스트 3개로 분리\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# rmse리스트\n",
    "average_rmse_values = []\n",
    "\n",
    "for train_index, test_index in kf.split(ratings):\n",
    "    # Training set과 test set을 나눔\n",
    "    train_data = ratings.iloc[train_index]\n",
    "    test_data = ratings.iloc[test_index]\n",
    "    \n",
    "    # np.array로 바꾸기\n",
    "    ratings_train = np.array(train_data.pivot(index='user_id', columns='isbn', values='rating').fillna(0))\n",
    "    ratings_test = np.array(test_data)\n",
    "    \n",
    "    # print(test_data)\n",
    "    \n",
    "    # Hybrid 결과 얻기\n",
    "    predictions = hybrid1(ratings_test[:, [0, 1]], [0.8, 0.2])\n",
    "    rmse=RMSE2(ratings_test[:, 2], predictions)\n",
    "    print(rmse)\n",
    "    average_rmse_values.append(rmse)\n",
    "\n",
    "# 전체 폴드에 대한 RMSE 평균 계산 및 출력\n",
    "average_rmse = np.mean(average_rmse_values)\n",
    "print(\"평균 RMSE:\", average_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hy2 1.1대 base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 1.399208 ; Test RMSE = 1.677623\n",
      "Iteration: 20 ; Train RMSE = 1.191494 ; Test RMSE = 1.665007\n",
      "Iteration: 30 ; Train RMSE = 0.978475 ; Test RMSE = 1.667352\n",
      "Iteration: 40 ; Train RMSE = 0.778754 ; Test RMSE = 1.673550\n",
      "20 1.664856511749714\n",
      "Iteration: 10 ; Train RMSE = 1.401221 ; Test RMSE = 1.671852\n",
      "Iteration: 20 ; Train RMSE = 1.192272 ; Test RMSE = 1.657453\n",
      "Iteration: 30 ; Train RMSE = 0.982934 ; Test RMSE = 1.659129\n",
      "Iteration: 40 ; Train RMSE = 0.786008 ; Test RMSE = 1.664173\n",
      "21 1.657168909333758\n",
      "Iteration: 10 ; Train RMSE = 1.409634 ; Test RMSE = 1.643798\n",
      "Iteration: 20 ; Train RMSE = 1.197614 ; Test RMSE = 1.633979\n",
      "Iteration: 30 ; Train RMSE = 0.987280 ; Test RMSE = 1.637629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 40 ; Train RMSE = 0.785657 ; Test RMSE = 1.644205\n",
      "20 1.6339147258262274\n",
      "평균 RMSE: 1.6519800489699\n",
      "Epoch 1/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 5.5418 - RMSE: 1.7541\n",
      "Epoch 1: val_RMSE improved from inf to 1.68318, saving model to CheckPoint\n",
      "186/186 [==============================] - 3s 9ms/step - loss: 5.5025 - RMSE: 1.7564 - val_loss: 1.8324 - val_RMSE: 1.6832\n",
      "Epoch 2/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 1.6743 - RMSE: 1.5431\n",
      "Epoch 2: val_RMSE improved from 1.68318 to 1.67454, saving model to CheckPoint\n",
      "186/186 [==============================] - 2s 8ms/step - loss: 1.6743 - RMSE: 1.5371 - val_loss: 1.7992 - val_RMSE: 1.6745\n",
      "Epoch 3/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 1.3637 - RMSE: 1.2361\n",
      "Epoch 3: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.3655 - RMSE: 1.2473 - val_loss: 1.8641 - val_RMSE: 1.7330\n",
      "Epoch 4/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.1641 - RMSE: 1.0351\n",
      "Epoch 4: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.1676 - RMSE: 1.0483 - val_loss: 1.8913 - val_RMSE: 1.7624\n",
      "Epoch 5/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0939 - RMSE: 0.9664\n",
      "Epoch 5: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.0977 - RMSE: 0.9668 - val_loss: 1.9101 - val_RMSE: 1.7816\n",
      "Epoch 6/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0449 - RMSE: 0.9193\n",
      "Epoch 6: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.0480 - RMSE: 0.9246 - val_loss: 1.9187 - val_RMSE: 1.7931\n",
      "Epoch 7/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0161 - RMSE: 0.8913\n",
      "Epoch 7: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 1.0173 - RMSE: 0.8888 - val_loss: 1.9275 - val_RMSE: 1.8023\n",
      "Epoch 8/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9890 - RMSE: 0.8657\n",
      "Epoch 8: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9897 - RMSE: 0.8644 - val_loss: 1.9433 - val_RMSE: 1.8199\n",
      "Epoch 9/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9730 - RMSE: 0.8523\n",
      "Epoch 9: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9744 - RMSE: 0.8506 - val_loss: 1.9424 - val_RMSE: 1.8214\n",
      "Epoch 10/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9619 - RMSE: 0.8432\n",
      "Epoch 10: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9627 - RMSE: 0.8412 - val_loss: 1.9538 - val_RMSE: 1.8347\n",
      "Epoch 11/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9464 - RMSE: 0.8295\n",
      "Epoch 11: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9483 - RMSE: 0.8351 - val_loss: 1.9543 - val_RMSE: 1.8369\n",
      "Epoch 12/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.9381 - RMSE: 0.8224\n",
      "Epoch 12: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9390 - RMSE: 0.8257 - val_loss: 1.9589 - val_RMSE: 1.8431\n",
      "Epoch 13/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9280 - RMSE: 0.8143\n",
      "Epoch 13: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9301 - RMSE: 0.8124 - val_loss: 1.9629 - val_RMSE: 1.8489\n",
      "Epoch 14/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.9174 - RMSE: 0.8061\n",
      "Epoch 14: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9185 - RMSE: 0.8029 - val_loss: 1.9648 - val_RMSE: 1.8535\n",
      "Epoch 15/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9083 - RMSE: 0.7993\n",
      "Epoch 15: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9103 - RMSE: 0.8006 - val_loss: 1.9658 - val_RMSE: 1.8568\n",
      "Epoch 16/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9023 - RMSE: 0.7959\n",
      "Epoch 16: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9051 - RMSE: 0.7952 - val_loss: 1.9768 - val_RMSE: 1.8703\n",
      "Epoch 17/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8914 - RMSE: 0.7882\n",
      "Epoch 17: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8929 - RMSE: 0.7860 - val_loss: 1.9685 - val_RMSE: 1.8657\n",
      "Epoch 18/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8847 - RMSE: 0.7836\n",
      "Epoch 18: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8846 - RMSE: 0.7816 - val_loss: 1.9684 - val_RMSE: 1.8666\n",
      "Epoch 19/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8795 - RMSE: 0.7797\n",
      "Epoch 19: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8793 - RMSE: 0.7763 - val_loss: 1.9715 - val_RMSE: 1.8714\n",
      "Epoch 20/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8742 - RMSE: 0.7755\n",
      "Epoch 20: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8743 - RMSE: 0.7803 - val_loss: 1.9764 - val_RMSE: 1.8771\n",
      "Epoch 21/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8685 - RMSE: 0.7713\n",
      "Epoch 21: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8692 - RMSE: 0.7772 - val_loss: 1.9807 - val_RMSE: 1.8836\n",
      "Epoch 22/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8577 - RMSE: 0.7630\n",
      "Epoch 22: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8598 - RMSE: 0.7626 - val_loss: 1.9717 - val_RMSE: 1.8767\n",
      "Epoch 23/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8541 - RMSE: 0.7610\n",
      "Epoch 23: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8551 - RMSE: 0.7607 - val_loss: 1.9744 - val_RMSE: 1.8810\n",
      "Epoch 24/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8486 - RMSE: 0.7577\n",
      "Epoch 24: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8494 - RMSE: 0.7567 - val_loss: 1.9790 - val_RMSE: 1.8880\n",
      "Epoch 25/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8429 - RMSE: 0.7537\n",
      "Epoch 25: val_RMSE did not improve from 1.67454\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8450 - RMSE: 0.7543 - val_loss: 1.9688 - val_RMSE: 1.8793\n",
      "371/371 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 5.5505 - RMSE: 1.7596\n",
      "Epoch 1: val_RMSE improved from inf to 1.66843, saving model to CheckPoint\n",
      "186/186 [==============================] - 3s 9ms/step - loss: 5.5091 - RMSE: 1.7598 - val_loss: 1.8278 - val_RMSE: 1.6684\n",
      "Epoch 2/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 1.6831 - RMSE: 1.5428\n",
      "Epoch 2: val_RMSE improved from 1.66843 to 1.66036, saving model to CheckPoint\n",
      "186/186 [==============================] - 2s 8ms/step - loss: 1.6832 - RMSE: 1.5469 - val_loss: 1.7941 - val_RMSE: 1.6604\n",
      "Epoch 3/25\n",
      "186/186 [==============================] - ETA: 0s - loss: 1.3713 - RMSE: 1.2304\n",
      "Epoch 3: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.3713 - RMSE: 1.2304 - val_loss: 1.8679 - val_RMSE: 1.7295\n",
      "Epoch 4/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.1699 - RMSE: 1.0352\n",
      "Epoch 4: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.1716 - RMSE: 1.0330 - val_loss: 1.8982 - val_RMSE: 1.7630\n",
      "Epoch 5/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 1.1006 - RMSE: 0.9683\n",
      "Epoch 5: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.1006 - RMSE: 0.9647 - val_loss: 1.9126 - val_RMSE: 1.7790\n",
      "Epoch 6/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0456 - RMSE: 0.9155\n",
      "Epoch 6: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 1.0479 - RMSE: 0.9196 - val_loss: 1.9208 - val_RMSE: 1.7901\n",
      "Epoch 7/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0119 - RMSE: 0.8848\n",
      "Epoch 7: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.0146 - RMSE: 0.8864 - val_loss: 1.9381 - val_RMSE: 1.8099\n",
      "Epoch 8/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9928 - RMSE: 0.8688\n",
      "Epoch 8: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9942 - RMSE: 0.8710 - val_loss: 1.9372 - val_RMSE: 1.8129\n",
      "Epoch 9/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9692 - RMSE: 0.8486\n",
      "Epoch 9: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9710 - RMSE: 0.8558 - val_loss: 1.9429 - val_RMSE: 1.8217\n",
      "Epoch 10/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9535 - RMSE: 0.8363\n",
      "Epoch 10: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9542 - RMSE: 0.8476 - val_loss: 1.9464 - val_RMSE: 1.8287\n",
      "Epoch 11/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.9407 - RMSE: 0.8265\n",
      "Epoch 11: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9416 - RMSE: 0.8264 - val_loss: 1.9364 - val_RMSE: 1.8218\n",
      "Epoch 12/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.9276 - RMSE: 0.8167\n",
      "Epoch 12: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9287 - RMSE: 0.8177 - val_loss: 1.9482 - val_RMSE: 1.8369\n",
      "Epoch 13/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9169 - RMSE: 0.8099\n",
      "Epoch 13: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9188 - RMSE: 0.8176 - val_loss: 1.9471 - val_RMSE: 1.8402\n",
      "Epoch 14/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9036 - RMSE: 0.8002\n",
      "Epoch 14: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9047 - RMSE: 0.7981 - val_loss: 1.9549 - val_RMSE: 1.8505\n",
      "Epoch 15/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8932 - RMSE: 0.7920\n",
      "Epoch 15: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8955 - RMSE: 0.7915 - val_loss: 1.9525 - val_RMSE: 1.8501\n",
      "Epoch 16/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.8869 - RMSE: 0.7882\n",
      "Epoch 16: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8874 - RMSE: 0.7849 - val_loss: 1.9524 - val_RMSE: 1.8530\n",
      "Epoch 17/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8775 - RMSE: 0.7811\n",
      "Epoch 17: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8790 - RMSE: 0.7797 - val_loss: 1.9677 - val_RMSE: 1.8700\n",
      "Epoch 18/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8699 - RMSE: 0.7757\n",
      "Epoch 18: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8704 - RMSE: 0.7751 - val_loss: 1.9528 - val_RMSE: 1.8575\n",
      "Epoch 19/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.8623 - RMSE: 0.7705\n",
      "Epoch 19: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8640 - RMSE: 0.7698 - val_loss: 1.9604 - val_RMSE: 1.8677\n",
      "Epoch 20/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8588 - RMSE: 0.7685\n",
      "Epoch 20: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8602 - RMSE: 0.7673 - val_loss: 1.9621 - val_RMSE: 1.8702\n",
      "Epoch 21/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8502 - RMSE: 0.7615\n",
      "Epoch 21: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8515 - RMSE: 0.7604 - val_loss: 1.9512 - val_RMSE: 1.8617\n",
      "Epoch 22/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8468 - RMSE: 0.7600\n",
      "Epoch 22: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8464 - RMSE: 0.7558 - val_loss: 1.9592 - val_RMSE: 1.8709\n",
      "Epoch 23/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.8395 - RMSE: 0.7539\n",
      "Epoch 23: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8398 - RMSE: 0.7503 - val_loss: 1.9565 - val_RMSE: 1.8697\n",
      "Epoch 24/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8287 - RMSE: 0.7453\n",
      "Epoch 24: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8327 - RMSE: 0.7584 - val_loss: 1.9686 - val_RMSE: 1.8846\n",
      "Epoch 25/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8277 - RMSE: 0.7471\n",
      "Epoch 25: val_RMSE did not improve from 1.66036\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8281 - RMSE: 0.7449 - val_loss: 1.9619 - val_RMSE: 1.8804\n",
      "371/371 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 5.5242 - RMSE: 1.7528\n",
      "Epoch 1: val_RMSE improved from inf to 1.68671, saving model to CheckPoint\n",
      "186/186 [==============================] - 3s 9ms/step - loss: 5.5041 - RMSE: 1.7470 - val_loss: 1.8457 - val_RMSE: 1.6867\n",
      "Epoch 2/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 1.6868 - RMSE: 1.5475\n",
      "Epoch 2: val_RMSE improved from 1.68671 to 1.65957, saving model to CheckPoint\n",
      "186/186 [==============================] - 2s 8ms/step - loss: 1.6868 - RMSE: 1.5478 - val_loss: 1.7913 - val_RMSE: 1.6596\n",
      "Epoch 3/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 1.3805 - RMSE: 1.2466\n",
      "Epoch 3: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.3813 - RMSE: 1.2496 - val_loss: 1.8649 - val_RMSE: 1.7284\n",
      "Epoch 4/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.1741 - RMSE: 1.0405\n",
      "Epoch 4: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.1755 - RMSE: 1.0386 - val_loss: 1.8727 - val_RMSE: 1.7395\n",
      "Epoch 5/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0990 - RMSE: 0.9676\n",
      "Epoch 5: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 1.1002 - RMSE: 0.9716 - val_loss: 1.8923 - val_RMSE: 1.7604\n",
      "Epoch 6/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0481 - RMSE: 0.9195\n",
      "Epoch 6: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 1.0492 - RMSE: 0.9181 - val_loss: 1.9034 - val_RMSE: 1.7750\n",
      "Epoch 7/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0172 - RMSE: 0.8907\n",
      "Epoch 7: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 1.0195 - RMSE: 0.8926 - val_loss: 1.9111 - val_RMSE: 1.7845\n",
      "Epoch 8/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9920 - RMSE: 0.8674\n",
      "Epoch 8: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9942 - RMSE: 0.8657 - val_loss: 1.9131 - val_RMSE: 1.7881\n",
      "Epoch 9/25\n",
      "179/186 [===========================>..] - ETA: 0s - loss: 0.9740 - RMSE: 0.8516\n",
      "Epoch 9: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9739 - RMSE: 0.8511 - val_loss: 1.9222 - val_RMSE: 1.8000\n",
      "Epoch 10/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9540 - RMSE: 0.8342\n",
      "Epoch 10: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9569 - RMSE: 0.8372 - val_loss: 1.9189 - val_RMSE: 1.7992\n",
      "Epoch 11/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9457 - RMSE: 0.8282\n",
      "Epoch 11: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.9477 - RMSE: 0.8272 - val_loss: 1.9314 - val_RMSE: 1.8137\n",
      "Epoch 12/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9291 - RMSE: 0.8142\n",
      "Epoch 12: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9297 - RMSE: 0.8117 - val_loss: 1.9278 - val_RMSE: 1.8127\n",
      "Epoch 13/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9224 - RMSE: 0.8099\n",
      "Epoch 13: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9235 - RMSE: 0.8143 - val_loss: 1.9267 - val_RMSE: 1.8144\n",
      "Epoch 14/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.9109 - RMSE: 0.8019\n",
      "Epoch 14: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9124 - RMSE: 0.8082 - val_loss: 1.9351 - val_RMSE: 1.8263\n",
      "Epoch 15/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8990 - RMSE: 0.7929\n",
      "Epoch 15: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.9002 - RMSE: 0.8001 - val_loss: 1.9403 - val_RMSE: 1.8342\n",
      "Epoch 16/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8926 - RMSE: 0.7893\n",
      "Epoch 16: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8934 - RMSE: 0.7949 - val_loss: 1.9406 - val_RMSE: 1.8374\n",
      "Epoch 17/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8829 - RMSE: 0.7824\n",
      "Epoch 17: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8845 - RMSE: 0.7844 - val_loss: 1.9489 - val_RMSE: 1.8484\n",
      "Epoch 18/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8726 - RMSE: 0.7753\n",
      "Epoch 18: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8741 - RMSE: 0.7750 - val_loss: 1.9383 - val_RMSE: 1.8410\n",
      "Epoch 19/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8679 - RMSE: 0.7727\n",
      "Epoch 19: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8692 - RMSE: 0.7736 - val_loss: 1.9479 - val_RMSE: 1.8521\n",
      "Epoch 20/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8593 - RMSE: 0.7654\n",
      "Epoch 20: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8619 - RMSE: 0.7686 - val_loss: 1.9455 - val_RMSE: 1.8509\n",
      "Epoch 21/25\n",
      "178/186 [===========================>..] - ETA: 0s - loss: 0.8508 - RMSE: 0.7582\n",
      "Epoch 21: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8561 - RMSE: 0.7613 - val_loss: 1.9459 - val_RMSE: 1.8528\n",
      "Epoch 22/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.8482 - RMSE: 0.7571\n",
      "Epoch 22: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8503 - RMSE: 0.7624 - val_loss: 1.9492 - val_RMSE: 1.8568\n",
      "Epoch 23/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.8439 - RMSE: 0.7536\n",
      "Epoch 23: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8435 - RMSE: 0.7545 - val_loss: 1.9561 - val_RMSE: 1.8649\n",
      "Epoch 24/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.8402 - RMSE: 0.7508\n",
      "Epoch 24: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 8ms/step - loss: 0.8413 - RMSE: 0.7508 - val_loss: 1.9548 - val_RMSE: 1.8646\n",
      "Epoch 25/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.8360 - RMSE: 0.7482\n",
      "Epoch 25: val_RMSE did not improve from 1.65957\n",
      "186/186 [==============================] - 1s 7ms/step - loss: 0.8360 - RMSE: 0.7500 - val_loss: 1.9488 - val_RMSE: 1.8609\n",
      "370/370 [==============================] - 1s 2ms/step\n",
      "평균 RMSE: 2.051749224023101\n",
      "371/371 [==============================] - 1s 2ms/step\n",
      "1.220380524785646\n",
      "1.1485235776459166\n",
      "1.4055210714780104\n",
      "Weights - 0.10 : 0.90 ; RMSE = 1.3612429\n",
      "Weights - 0.20 : 0.80 ; RMSE = 1.3202699\n",
      "Weights - 0.30 : 0.70 ; RMSE = 1.2829187\n",
      "Weights - 0.40 : 0.60 ; RMSE = 1.2495142\n",
      "Weights - 0.50 : 0.50 ; RMSE = 1.2203805\n",
      "Weights - 0.60 : 0.40 ; RMSE = 1.1958299\n",
      "Weights - 0.70 : 0.30 ; RMSE = 1.1761493\n",
      "Weights - 0.80 : 0.20 ; RMSE = 1.1615863\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1523349\n",
      "Weights - 0.88 : 0.12 ; RMSE = 1.1537531\n",
      "Weights - 0.89 : 0.11 ; RMSE = 1.1530168\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1523349\n",
      "Weights - 0.91 : 0.09 ; RMSE = 1.1517075\n",
      "Weights - 0.92 : 0.08 ; RMSE = 1.1511346\n",
      "Weights - 0.93 : 0.07 ; RMSE = 1.1506164\n",
      "Weights - 0.94 : 0.06 ; RMSE = 1.1501528\n",
      "Weights - 0.95 : 0.05 ; RMSE = 1.1497441\n",
      "Weights - 0.96 : 0.04 ; RMSE = 1.1493901\n",
      "Weights - 0.97 : 0.03 ; RMSE = 1.1490911\n",
      "Weights - 0.98 : 0.02 ; RMSE = 1.1488469\n",
      "Weights - 0.99 : 0.01 ; RMSE = 1.1486578\n",
      "371/371 [==============================] - 1s 2ms/step\n",
      "1.2224033814361266\n",
      "1.1556635346179498\n",
      "1.3988553742472598\n",
      "Weights - 0.10 : 0.90 ; RMSE = 1.3564608\n",
      "Weights - 0.20 : 0.80 ; RMSE = 1.3173185\n",
      "Weights - 0.30 : 0.70 ; RMSE = 1.2817265\n",
      "Weights - 0.40 : 0.60 ; RMSE = 1.2499881\n",
      "Weights - 0.50 : 0.50 ; RMSE = 1.2224034\n",
      "Weights - 0.60 : 0.40 ; RMSE = 1.1992591\n",
      "Weights - 0.70 : 0.30 ; RMSE = 1.1808165\n",
      "Weights - 0.80 : 0.20 ; RMSE = 1.1672982\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1588768\n",
      "Weights - 0.88 : 0.12 ; RMSE = 1.1601470\n",
      "Weights - 0.89 : 0.11 ; RMSE = 1.1594859\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1588768\n",
      "Weights - 0.91 : 0.09 ; RMSE = 1.1583198\n",
      "Weights - 0.92 : 0.08 ; RMSE = 1.1578151\n",
      "Weights - 0.93 : 0.07 ; RMSE = 1.1573626\n",
      "Weights - 0.94 : 0.06 ; RMSE = 1.1569625\n",
      "Weights - 0.95 : 0.05 ; RMSE = 1.1566148\n",
      "Weights - 0.96 : 0.04 ; RMSE = 1.1563195\n",
      "Weights - 0.97 : 0.03 ; RMSE = 1.1560767\n",
      "Weights - 0.98 : 0.02 ; RMSE = 1.1558864\n",
      "Weights - 0.99 : 0.01 ; RMSE = 1.1557487\n",
      "370/370 [==============================] - 1s 2ms/step\n",
      "1.2044875159292316\n",
      "1.1327583118961548\n",
      "1.388235954578367\n",
      "Weights - 0.10 : 0.90 ; RMSE = 1.3443275\n",
      "Weights - 0.20 : 0.80 ; RMSE = 1.3036811\n",
      "Weights - 0.30 : 0.70 ; RMSE = 1.2666106\n",
      "Weights - 0.40 : 0.60 ; RMSE = 1.2334387\n",
      "Weights - 0.50 : 0.50 ; RMSE = 1.2044875\n",
      "Weights - 0.60 : 0.40 ; RMSE = 1.1800676\n",
      "Weights - 0.70 : 0.30 ; RMSE = 1.1604652\n",
      "Weights - 0.80 : 0.20 ; RMSE = 1.1459274\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1366487\n",
      "Weights - 0.88 : 0.12 ; RMSE = 1.1380765\n",
      "Weights - 0.89 : 0.11 ; RMSE = 1.1373357\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1366487\n",
      "Weights - 0.91 : 0.09 ; RMSE = 1.1360157\n",
      "Weights - 0.92 : 0.08 ; RMSE = 1.1354367\n",
      "Weights - 0.93 : 0.07 ; RMSE = 1.1349118\n",
      "Weights - 0.94 : 0.06 ; RMSE = 1.1344411\n",
      "Weights - 0.95 : 0.05 ; RMSE = 1.1340247\n",
      "Weights - 0.96 : 0.04 ; RMSE = 1.1336626\n",
      "Weights - 0.97 : 0.03 ; RMSE = 1.1333549\n",
      "Weights - 0.98 : 0.02 ; RMSE = 1.1331016\n",
      "Weights - 0.99 : 0.01 ; RMSE = 1.1329027\n"
     ]
    }
   ],
   "source": [
    "# Predictions using MF ###########################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path='/home/recordk/reco/BX-Book-Ratings.csv'\n",
    "ratings = pd.read_csv(path)\n",
    "ratings['Book-Rating'] = ratings['Book-Rating'].astype(int)\n",
    "ratings.columns=['user_id','isbn','rating']\n",
    "ratings=ratings[ratings['rating']!=0]\n",
    "ratings=ratings.reset_index(drop=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# LabelEncoder\n",
    "user_encoder = LabelEncoder()\n",
    "isbn_encoder = LabelEncoder()\n",
    "\n",
    "# user_id와 isbn label인코딩\n",
    "ratings['user_id'] = user_encoder.fit_transform(ratings['user_id'])\n",
    "ratings['isbn'] = isbn_encoder.fit_transform(ratings['isbn'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NEW_MF():\n",
    "    def __init__(self, ratings, K, alpha, beta, iterations, tolerance=0.005, verbose=True):\n",
    "        self.R = np.array(ratings)\n",
    "\n",
    "        item_id_index = []\n",
    "        index_item_id = []\n",
    "        for i, one_id in enumerate(ratings):\n",
    "            item_id_index.append([one_id, i])\n",
    "            index_item_id.append([i, one_id])\n",
    "        self.item_id_index = dict(item_id_index)\n",
    "        self.index_item_id = dict(index_item_id)        \n",
    "        user_id_index = []\n",
    "        index_user_id = []\n",
    "        for i, one_id in enumerate(ratings.T):\n",
    "            user_id_index.append([one_id, i])\n",
    "            index_user_id.append([i, one_id])\n",
    "        self.user_id_index = dict(user_id_index)\n",
    "        self.index_user_id = dict(index_user_id)\n",
    "        # 다른 변수 초기화\n",
    "        self.num_users, self.num_items = np.shape(self.R)\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        # print(len(self.user_id_index))\n",
    "    # 테스트 셋을 선정하는 메소드 \n",
    "    def set_test(self, ratings_test):                           # Setting test set\n",
    "        test_set = []\n",
    "        for i in range(len(ratings_test)):                      # Selected ratings\n",
    "            # print(self.user_id_index)\n",
    "            # print(self.user_id_index[4738])\n",
    "            x = self.user_id_index[ratings_test.iloc[i,0]]      # Getting R indice for the given user_id and isbn\n",
    "            y = self.item_id_index[ratings_test.iloc[i,1]]\n",
    "            z = ratings_test.iloc[i,2]\n",
    "            test_set.append([x, y, z])\n",
    "            self.R[x, y] = 0                    # Setting test set ratings to 0\n",
    "        self.test_set = test_set\n",
    "        return test_set                         # Return test set\n",
    "\n",
    "    def test(self):                             # Training 하면서 test set의 정확도를 계산하는 메소드 \n",
    "        # user isbn 행렬 선언\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        # bias 선언\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[self.R.nonzero()])\n",
    "\n",
    "        # 학습리스트\n",
    "        rows, columns = self.R.nonzero()\n",
    "        self.samples = [(i,j, self.R[i,j]) for i, j in zip(rows, columns)]\n",
    "\n",
    "        # sgd iter\n",
    "        best_RMSE = 10000\n",
    "        best_iteration = 0\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            rmse1 = self.rmse()\n",
    "            rmse2 = self.test_rmse()\n",
    "            training_process.append((i, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1, rmse1, rmse2))\n",
    "            if best_RMSE > rmse2:                      # New best record\n",
    "                best_RMSE = rmse2\n",
    "                best_iteration = i\n",
    "            elif (rmse2 - best_RMSE) > self.tolerance: # RMSE is increasing over tolerance\n",
    "                break\n",
    "        print(best_iteration, best_RMSE)\n",
    "        return training_process,best_RMSE\n",
    "\n",
    "    # sgd\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction = self.get_prediction(i, j)\n",
    "            error = (r - prediction)\n",
    "            self.b_u[i] += self.alpha * (error - self.beta * self.b_u[i])\n",
    "            self.b_d[j] += self.alpha * (error - self.beta * self.b_d[j])\n",
    "\n",
    "            self.Q[j, :] += self.alpha * (error * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "            self.P[i, :] += self.alpha * (error * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "\n",
    "    # train rmse\n",
    "    def rmse(self):\n",
    "        rows, columns = self.R.nonzero()\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        for x, y in zip(rows, columns):\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "\n",
    "    # Test RMSE 계산하는 method \n",
    "    def test_rmse(self):\n",
    "        error = 0\n",
    "        for one_set in self.test_set:\n",
    "            predicted = self.get_prediction(one_set[0], one_set[1])\n",
    "            error += pow(one_set[2] - predicted, 2)\n",
    "        return np.sqrt(error/len(self.test_set))\n",
    "\n",
    "    # 평점 유저, isbn\n",
    "    def get_prediction(self, i, j):\n",
    "        prediction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "     # 평점 유저, isbn\n",
    "    def get_one_prediction(self, user_id, isbn):\n",
    "        return self.get_prediction(self.user_id_index[user_id], self.item_id_index[isbn])\n",
    "\n",
    "\n",
    "# kfold이용해서 트레인 테스트 3개로 분리\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# rmse리스트\n",
    "average_rmse_values = []\n",
    "\n",
    "for train_index, test_index in kf.split(ratings):\n",
    "    # Training set과 test set을 나눔\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "    train_data = ratings.iloc[train_index]\n",
    "    test_data = ratings.iloc[test_index]\n",
    "    # print(test_data)\n",
    "    # 모델 생성 및 훈련\n",
    "    temp = ratings.pivot(index = 'user_id', columns ='isbn', values = 'rating').fillna(0)\n",
    "    # mf = NEW_MF(temp, K=220, alpha=0.0014, beta=0.075, iterations=350, tolerance=0.0001, verbose=True)\n",
    "    mf = NEW_MF(temp, K=30, alpha=0.01, beta=0.02, iterations=100, tolerance=0.01, verbose=True)\n",
    "    # Test set에 대한 예측 및 평가\n",
    "    test_set = mf.set_test(test_data)\n",
    "    result,rmse = mf.test()\n",
    "    average_rmse_values.append(rmse)\n",
    "\n",
    "# 전체 폴드에 대한 RMSE 평균 계산 및 출력\n",
    "average_rmse = np.mean(average_rmse_values)\n",
    "print(\"평균 RMSE:\", average_rmse)\n",
    "\n",
    "\n",
    "# Predictions using DL ###########################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.layers import Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "# Variable 초기화 \n",
    "K = 350                         # Latent factor 수 \n",
    "lr = 0.01                           # 학습률\n",
    "reg = 0.005                     # Regularization penalty\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "# 모델 구조\n",
    "def architecture(train_data, test_data,lr):\n",
    "    # Keras model\n",
    "    user = Input(shape=(1,))  # User input\n",
    "    item = Input(shape=(1,))  # Item input\n",
    "    P_embedding = Embedding(N, K, embeddings_regularizer=l2(reg))(user)  # (N, 1, K)\n",
    "    Q_embedding = Embedding(M, K, embeddings_regularizer=l2(reg))(item)  # (M, 1, K)\n",
    "    user_bias = Embedding(N, 1, embeddings_regularizer=l2(reg))(user)  # User bias term (N, 1, )\n",
    "    item_bias = Embedding(M, 1, embeddings_regularizer=l2(reg))(item)  # Item bias term (M, 1, )\n",
    "\n",
    "    # Concatenate layers\n",
    "    P_embedding = Flatten()(P_embedding)  # (K, )\n",
    "    Q_embedding = Flatten()(Q_embedding)  # (K, )\n",
    "    user_bias = Flatten()(user_bias)  # (1, )\n",
    "    item_bias = Flatten()(item_bias)  # (1, )\n",
    "    R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias])  # (2K + 2, )\n",
    "\n",
    "    # Neural network\n",
    "    R = Dense(2048)(R)\n",
    "    R = Activation('swish')(R)\n",
    "    R = Dense(1)(R)\n",
    "\n",
    "    model = Model(inputs=[user, item], outputs=R)\n",
    "    model.compile(\n",
    "        loss=RMSE,\n",
    "        #optimizer=SGD(lr=0.1, momentum=0.9),\n",
    "        optimizer=Adamax(lr=lr),\n",
    "        metrics=[RMSE]\n",
    "    )\n",
    "\n",
    "    checkpoint_path = 'CheckPoint'\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True,\n",
    "                                monitor='val_RMSE',\n",
    "                                verbose=1)\n",
    "\n",
    "    result = model.fit(\n",
    "        x=[train_data.user_id.values, train_data.isbn.values],\n",
    "        y=train_data.rating.values - mu,\n",
    "        callbacks=[checkpoint],\n",
    "        epochs=25,\n",
    "        batch_size=128,\n",
    "        validation_data=(\n",
    "            [test_data.user_id.values, test_data.isbn.values],\n",
    "            test_data.rating.values - mu\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model, result\n",
    "\n",
    "# 사용자 수 및 도서 수\n",
    "N = len(set(ratings.user_id)) + 1\n",
    "M = len(set(ratings.isbn)) + 1\n",
    "\n",
    "\n",
    "# kfold이용해서 트레인 테스트 3개로 분리\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# rmse리스트\n",
    "rmse_values = []\n",
    "\n",
    "# for문\n",
    "for train_indices, test_indices in kf.split(ratings):\n",
    "    train_data = ratings.iloc[train_indices].reset_index(drop=True)\n",
    "    test_data = ratings.iloc[test_indices].reset_index(drop=True)\n",
    "    \n",
    "    mu = train_data.rating.mean()    # 전체 평균 \n",
    "\n",
    "    # 셔플\n",
    "    train_data = shuffle(train_data)\n",
    "\n",
    "    # 모델 및 결과\n",
    "    model, result = architecture(train_data, test_data,lr)\n",
    "\n",
    "    # 모델 평가\n",
    "    predictions = model.predict([test_data.user_id.values, test_data.isbn.values]) + mu\n",
    "    rmse = RMSE(test_data.rating.values, predictions)\n",
    "    rmse_values.append(rmse)\n",
    "\n",
    "# rmse 평균\n",
    "average_rmse = np.mean(rmse_values)\n",
    "print(\"평균 RMSE:\", average_rmse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hybrid recommendation ###########################################################################\n",
    "def recommender0(recomm_list, mf):\n",
    "    id_pairs = zip(recomm_list[:, 0], recomm_list[:, 1])\n",
    "    recommendations = np.array([mf.get_one_prediction(user, isbn) for (user, isbn) in id_pairs])\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Recommender 1\n",
    "def recommender1(recomm_list, model):\n",
    "    user_ids = recomm_list[:, 0]\n",
    "    isbn = recomm_list[:, 1]\n",
    "    recommendations = model.predict([user_ids, isbn]) + mu\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# RMSE 계산을 위한 함수\n",
    "def RMSE2(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred)) ** 2))\n",
    "\n",
    "# kfold이용해서 트레인 테스트 3개로 분리\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# rmse리스트\n",
    "rmse_values = []\n",
    "\n",
    "# for문\n",
    "for train_indices, test_indices in kf.split(ratings):\n",
    "    train_data = ratings.iloc[train_indices].reset_index(drop=True)\n",
    "    test_data = ratings.iloc[test_indices].reset_index(drop=True)\n",
    "    recomm_list = np.array(test_data.iloc[:, [0, 1]])\n",
    "    result0 = recommender0(recomm_list, mf)\n",
    "    result1 = np.ravel(recommender1(recomm_list, model))\n",
    "\n",
    "    weight = [0.5, 0.5]\n",
    "    predictions = []\n",
    "    for i, number in enumerate(result0):\n",
    "        predictions.append(result0[i] * weight[0] + result1[i] * weight[1])\n",
    "    print(RMSE2(test_data['rating'], predictions))\n",
    "    print(RMSE2(test_data['rating'], result0))\n",
    "    print(RMSE2(test_data['rating'], result1))\n",
    "\n",
    "    for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        weight = [i, 1 - i]\n",
    "        predictions = []\n",
    "        for i, number in enumerate(result0):\n",
    "            predictions.append(result0[i] * weight[0] + result1[i] * weight[1])\n",
    "        print(\"Weights - %.2f : %.2f ; RMSE = %.7f\" % (weight[0], weight[1], RMSE2(test_data['rating'], predictions)))\n",
    "\n",
    "    for i in [0.88, 0.89, 0.90, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]:\n",
    "        weight = [i, 1 - i]\n",
    "        predictions = []\n",
    "        for i, number in enumerate(result0):\n",
    "            predictions.append(result0[i] * weight[0] + result1[i] * weight[1])\n",
    "        print(\"Weights - %.2f : %.2f ; RMSE = %.7f\" % (weight[0], weight[1], RMSE2(test_data['rating'], predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hy2 1.1대 swish 층 3개 각주O\n",
    " - RMSE 1.1475490666666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 1.397127 ; Test RMSE = 1.682567\n",
      "Iteration: 20 ; Train RMSE = 1.189229 ; Test RMSE = 1.670560\n",
      "Iteration: 30 ; Train RMSE = 0.978470 ; Test RMSE = 1.673958\n",
      "Iteration: 40 ; Train RMSE = 0.779834 ; Test RMSE = 1.680920\n",
      "20 1.6705488449249313\n",
      "Iteration: 10 ; Train RMSE = 1.410151 ; Test RMSE = 1.648082\n",
      "Iteration: 20 ; Train RMSE = 1.196928 ; Test RMSE = 1.637061\n",
      "Iteration: 30 ; Train RMSE = 0.983315 ; Test RMSE = 1.640656\n",
      "Iteration: 40 ; Train RMSE = 0.784239 ; Test RMSE = 1.647733\n",
      "19 1.637060536177332\n",
      "Iteration: 10 ; Train RMSE = 1.400140 ; Test RMSE = 1.670089\n",
      "Iteration: 20 ; Train RMSE = 1.190195 ; Test RMSE = 1.658814\n",
      "Iteration: 30 ; Train RMSE = 0.981148 ; Test RMSE = 1.661657\n",
      "Iteration: 40 ; Train RMSE = 0.782620 ; Test RMSE = 1.668159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 1.6587715442839177\n",
      "평균 RMSE: 1.6554603084620603\n",
      "Epoch 1/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 5.3092 - RMSE: 1.7713\n",
      "Epoch 1: val_RMSE improved from inf to 1.67890, saving model to CheckPoint\n",
      "186/186 [==============================] - 5s 11ms/step - loss: 5.2713 - RMSE: 1.7689 - val_loss: 1.7140 - val_RMSE: 1.6789\n",
      "Epoch 2/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.5969 - RMSE: 1.5462\n",
      "Epoch 2: val_RMSE improved from 1.67890 to 1.65617, saving model to CheckPoint\n",
      "186/186 [==============================] - 2s 10ms/step - loss: 1.5973 - RMSE: 1.5483 - val_loss: 1.7187 - val_RMSE: 1.6562\n",
      "Epoch 3/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 1.2816 - RMSE: 1.2120\n",
      "Epoch 3: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.2827 - RMSE: 1.2205 - val_loss: 1.8121 - val_RMSE: 1.7325\n",
      "Epoch 4/25\n",
      "186/186 [==============================] - ETA: 0s - loss: 1.1153 - RMSE: 1.0312\n",
      "Epoch 4: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.1153 - RMSE: 1.0312 - val_loss: 1.8370 - val_RMSE: 1.7503\n",
      "Epoch 5/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 1.0457 - RMSE: 0.9585\n",
      "Epoch 5: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.0481 - RMSE: 0.9585 - val_loss: 1.8450 - val_RMSE: 1.7533\n",
      "Epoch 6/25\n",
      "186/186 [==============================] - ETA: 0s - loss: 1.0021 - RMSE: 0.9060\n",
      "Epoch 6: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.0021 - RMSE: 0.9060 - val_loss: 1.8829 - val_RMSE: 1.7870\n",
      "Epoch 7/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.9757 - RMSE: 0.8808\n",
      "Epoch 7: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9794 - RMSE: 0.8812 - val_loss: 1.8804 - val_RMSE: 1.7824\n",
      "Epoch 8/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.9622 - RMSE: 0.8647\n",
      "Epoch 8: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9622 - RMSE: 0.8694 - val_loss: 1.8973 - val_RMSE: 1.7973\n",
      "Epoch 9/25\n",
      "179/186 [===========================>..] - ETA: 0s - loss: 0.9389 - RMSE: 0.8410\n",
      "Epoch 9: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9458 - RMSE: 0.8445 - val_loss: 1.9077 - val_RMSE: 1.8080\n",
      "Epoch 10/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.9332 - RMSE: 0.8354\n",
      "Epoch 10: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9337 - RMSE: 0.8338 - val_loss: 1.9095 - val_RMSE: 1.8095\n",
      "Epoch 11/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.9236 - RMSE: 0.8260\n",
      "Epoch 11: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9238 - RMSE: 0.8237 - val_loss: 1.9317 - val_RMSE: 1.8330\n",
      "Epoch 12/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.9127 - RMSE: 0.8173\n",
      "Epoch 12: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9127 - RMSE: 0.8135 - val_loss: 1.9246 - val_RMSE: 1.8279\n",
      "Epoch 13/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.9001 - RMSE: 0.8062\n",
      "Epoch 13: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9006 - RMSE: 0.8066 - val_loss: 1.9389 - val_RMSE: 1.8438\n",
      "Epoch 14/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.8937 - RMSE: 0.8007\n",
      "Epoch 14: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8937 - RMSE: 0.7967 - val_loss: 1.9517 - val_RMSE: 1.8567\n",
      "Epoch 15/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8793 - RMSE: 0.7869\n",
      "Epoch 15: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8802 - RMSE: 0.7978 - val_loss: 1.9545 - val_RMSE: 1.8607\n",
      "Epoch 16/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.8770 - RMSE: 0.7865\n",
      "Epoch 16: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8770 - RMSE: 0.7901 - val_loss: 1.9541 - val_RMSE: 1.8629\n",
      "Epoch 17/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.8669 - RMSE: 0.7792\n",
      "Epoch 17: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8681 - RMSE: 0.7791 - val_loss: 1.9517 - val_RMSE: 1.8638\n",
      "Epoch 18/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.8621 - RMSE: 0.7777\n",
      "Epoch 18: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8618 - RMSE: 0.7740 - val_loss: 1.9445 - val_RMSE: 1.8593\n",
      "Epoch 19/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.8515 - RMSE: 0.7686\n",
      "Epoch 19: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8535 - RMSE: 0.7747 - val_loss: 1.9541 - val_RMSE: 1.8700\n",
      "Epoch 20/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.8434 - RMSE: 0.7616\n",
      "Epoch 20: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8435 - RMSE: 0.7585 - val_loss: 1.9544 - val_RMSE: 1.8711\n",
      "Epoch 21/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.8401 - RMSE: 0.7591\n",
      "Epoch 21: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8403 - RMSE: 0.7567 - val_loss: 1.9654 - val_RMSE: 1.8827\n",
      "Epoch 22/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8301 - RMSE: 0.7489\n",
      "Epoch 22: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8320 - RMSE: 0.7472 - val_loss: 1.9694 - val_RMSE: 1.8861\n",
      "Epoch 23/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.8319 - RMSE: 0.7501\n",
      "Epoch 23: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8332 - RMSE: 0.7477 - val_loss: 1.9804 - val_RMSE: 1.8965\n",
      "Epoch 24/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.8270 - RMSE: 0.7447\n",
      "Epoch 24: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8274 - RMSE: 0.7517 - val_loss: 1.9689 - val_RMSE: 1.8846\n",
      "Epoch 25/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.8184 - RMSE: 0.7361\n",
      "Epoch 25: val_RMSE did not improve from 1.65617\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8205 - RMSE: 0.7345 - val_loss: 1.9852 - val_RMSE: 1.9016\n",
      "371/371 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 08:03:48.569164: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1121674248 exceeds 10% of free system memory.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 5.3026 - RMSE: 1.7609\n",
      "Epoch 1: val_RMSE improved from inf to 1.70096, saving model to CheckPoint\n",
      "186/186 [==============================] - 3s 10ms/step - loss: 5.2635 - RMSE: 1.7624 - val_loss: 1.7383 - val_RMSE: 1.7010\n",
      "Epoch 2/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 1.5734 - RMSE: 1.5226\n",
      "Epoch 2: val_RMSE improved from 1.70096 to 1.67846, saving model to CheckPoint\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.5737 - RMSE: 1.5170 - val_loss: 1.7412 - val_RMSE: 1.6785\n",
      "Epoch 3/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 1.2606 - RMSE: 1.1919\n",
      "Epoch 3: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.2607 - RMSE: 1.1913 - val_loss: 1.8193 - val_RMSE: 1.7411\n",
      "Epoch 4/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 1.0978 - RMSE: 1.0179\n",
      "Epoch 4: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.0991 - RMSE: 1.0178 - val_loss: 1.8540 - val_RMSE: 1.7687\n",
      "Epoch 5/25\n",
      "186/186 [==============================] - ETA: 0s - loss: 1.0312 - RMSE: 0.9453\n",
      "Epoch 5: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.0312 - RMSE: 0.9453 - val_loss: 1.8673 - val_RMSE: 1.7768\n",
      "Epoch 6/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.9949 - RMSE: 0.9038\n",
      "Epoch 6: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9964 - RMSE: 0.9120 - val_loss: 1.8885 - val_RMSE: 1.7934\n",
      "Epoch 7/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.9609 - RMSE: 0.8665\n",
      "Epoch 7: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9647 - RMSE: 0.8719 - val_loss: 1.9124 - val_RMSE: 1.8145\n",
      "Epoch 8/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.9407 - RMSE: 0.8443\n",
      "Epoch 8: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9416 - RMSE: 0.8413 - val_loss: 1.9160 - val_RMSE: 1.8167\n",
      "Epoch 9/25\n",
      "186/186 [==============================] - ETA: 0s - loss: 0.9334 - RMSE: 0.8342\n",
      "Epoch 9: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9334 - RMSE: 0.8342 - val_loss: 1.9182 - val_RMSE: 1.8185\n",
      "Epoch 10/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.9223 - RMSE: 0.8245\n",
      "Epoch 10: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9226 - RMSE: 0.8227 - val_loss: 1.9325 - val_RMSE: 1.8326\n",
      "Epoch 11/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.9067 - RMSE: 0.8084\n",
      "Epoch 11: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9096 - RMSE: 0.8109 - val_loss: 1.9491 - val_RMSE: 1.8485\n",
      "Epoch 12/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.9002 - RMSE: 0.8022\n",
      "Epoch 12: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8996 - RMSE: 0.8024 - val_loss: 1.9615 - val_RMSE: 1.8619\n",
      "Epoch 13/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.8918 - RMSE: 0.7951\n",
      "Epoch 13: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8927 - RMSE: 0.8000 - val_loss: 1.9567 - val_RMSE: 1.8581\n",
      "Epoch 14/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.8837 - RMSE: 0.7881\n",
      "Epoch 14: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8837 - RMSE: 0.7841 - val_loss: 1.9661 - val_RMSE: 1.8684\n",
      "Epoch 15/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.8721 - RMSE: 0.7784\n",
      "Epoch 15: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8734 - RMSE: 0.7817 - val_loss: 1.9595 - val_RMSE: 1.8647\n",
      "Epoch 16/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.8637 - RMSE: 0.7718\n",
      "Epoch 16: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8645 - RMSE: 0.7780 - val_loss: 1.9652 - val_RMSE: 1.8718\n",
      "Epoch 17/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.8559 - RMSE: 0.7660\n",
      "Epoch 17: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8559 - RMSE: 0.7646 - val_loss: 1.9702 - val_RMSE: 1.8791\n",
      "Epoch 18/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8464 - RMSE: 0.7579\n",
      "Epoch 18: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8490 - RMSE: 0.7566 - val_loss: 1.9751 - val_RMSE: 1.8845\n",
      "Epoch 19/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.8419 - RMSE: 0.7542\n",
      "Epoch 19: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8419 - RMSE: 0.7512 - val_loss: 1.9787 - val_RMSE: 1.8893\n",
      "Epoch 20/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.8339 - RMSE: 0.7477\n",
      "Epoch 20: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8344 - RMSE: 0.7458 - val_loss: 1.9868 - val_RMSE: 1.8984\n",
      "Epoch 21/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.8278 - RMSE: 0.7428\n",
      "Epoch 21: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8297 - RMSE: 0.7418 - val_loss: 1.9838 - val_RMSE: 1.8966\n",
      "Epoch 22/25\n",
      "179/186 [===========================>..] - ETA: 0s - loss: 0.8234 - RMSE: 0.7383\n",
      "Epoch 22: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8250 - RMSE: 0.7401 - val_loss: 1.9987 - val_RMSE: 1.9108\n",
      "Epoch 23/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.8179 - RMSE: 0.7331\n",
      "Epoch 23: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8208 - RMSE: 0.7334 - val_loss: 1.9922 - val_RMSE: 1.9058\n",
      "Epoch 24/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.8163 - RMSE: 0.7325\n",
      "Epoch 24: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8171 - RMSE: 0.7326 - val_loss: 1.9834 - val_RMSE: 1.8976\n",
      "Epoch 25/25\n",
      "179/186 [===========================>..] - ETA: 0s - loss: 0.8072 - RMSE: 0.7249\n",
      "Epoch 25: val_RMSE did not improve from 1.67846\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8098 - RMSE: 0.7243 - val_loss: 1.9909 - val_RMSE: 1.9070\n",
      "371/371 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 08:04:33.750147: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1121674248 exceeds 10% of free system memory.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 5.2980 - RMSE: 1.7771\n",
      "Epoch 1: val_RMSE improved from inf to 1.67368, saving model to CheckPoint\n",
      "186/186 [==============================] - 3s 10ms/step - loss: 5.2793 - RMSE: 1.7792 - val_loss: 1.7062 - val_RMSE: 1.6737\n",
      "Epoch 2/25\n",
      "179/186 [===========================>..] - ETA: 0s - loss: 1.6007 - RMSE: 1.5518\n",
      "Epoch 2: val_RMSE improved from 1.67368 to 1.64594, saving model to CheckPoint\n",
      "186/186 [==============================] - 2s 10ms/step - loss: 1.6010 - RMSE: 1.5478 - val_loss: 1.7055 - val_RMSE: 1.6459\n",
      "Epoch 3/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 1.2878 - RMSE: 1.2193\n",
      "Epoch 3: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.2881 - RMSE: 1.2163 - val_loss: 1.7985 - val_RMSE: 1.7207\n",
      "Epoch 4/25\n",
      "182/186 [============================>.] - ETA: 0s - loss: 1.1144 - RMSE: 1.0345\n",
      "Epoch 4: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.1143 - RMSE: 1.0379 - val_loss: 1.8087 - val_RMSE: 1.7250\n",
      "Epoch 5/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 1.0440 - RMSE: 0.9579\n",
      "Epoch 5: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.0448 - RMSE: 0.9614 - val_loss: 1.8423 - val_RMSE: 1.7526\n",
      "Epoch 6/25\n",
      "179/186 [===========================>..] - ETA: 0s - loss: 0.9978 - RMSE: 0.9069\n",
      "Epoch 6: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 1.0025 - RMSE: 0.9208 - val_loss: 1.8599 - val_RMSE: 1.7668\n",
      "Epoch 7/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.9714 - RMSE: 0.8775\n",
      "Epoch 7: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9750 - RMSE: 0.8801 - val_loss: 1.8704 - val_RMSE: 1.7750\n",
      "Epoch 8/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.9496 - RMSE: 0.8544\n",
      "Epoch 8: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9512 - RMSE: 0.8540 - val_loss: 1.8771 - val_RMSE: 1.7802\n",
      "Epoch 9/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.9346 - RMSE: 0.8380\n",
      "Epoch 9: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9374 - RMSE: 0.8445 - val_loss: 1.8888 - val_RMSE: 1.7918\n",
      "Epoch 10/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.9241 - RMSE: 0.8281\n",
      "Epoch 10: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9242 - RMSE: 0.8289 - val_loss: 1.9072 - val_RMSE: 1.8106\n",
      "Epoch 11/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.9083 - RMSE: 0.8131\n",
      "Epoch 11: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9101 - RMSE: 0.8201 - val_loss: 1.9130 - val_RMSE: 1.8170\n",
      "Epoch 12/25\n",
      "179/186 [===========================>..] - ETA: 0s - loss: 0.9017 - RMSE: 0.8070\n",
      "Epoch 12: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.9036 - RMSE: 0.8101 - val_loss: 1.9143 - val_RMSE: 1.8192\n",
      "Epoch 13/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.8873 - RMSE: 0.7938\n",
      "Epoch 13: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8901 - RMSE: 0.7969 - val_loss: 1.9268 - val_RMSE: 1.8325\n",
      "Epoch 14/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.8856 - RMSE: 0.7933\n",
      "Epoch 14: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8856 - RMSE: 0.7911 - val_loss: 1.9268 - val_RMSE: 1.8344\n",
      "Epoch 15/25\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.8689 - RMSE: 0.7780\n",
      "Epoch 15: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8689 - RMSE: 0.7764 - val_loss: 1.9113 - val_RMSE: 1.8203\n",
      "Epoch 16/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.8570 - RMSE: 0.7676\n",
      "Epoch 16: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8600 - RMSE: 0.7731 - val_loss: 1.9457 - val_RMSE: 1.8556\n",
      "Epoch 17/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.8489 - RMSE: 0.7605\n",
      "Epoch 17: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8521 - RMSE: 0.7661 - val_loss: 1.9254 - val_RMSE: 1.8374\n",
      "Epoch 18/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.8447 - RMSE: 0.7585\n",
      "Epoch 18: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8450 - RMSE: 0.7551 - val_loss: 1.9367 - val_RMSE: 1.8509\n",
      "Epoch 19/25\n",
      "186/186 [==============================] - ETA: 0s - loss: 0.8353 - RMSE: 0.7553\n",
      "Epoch 19: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8353 - RMSE: 0.7553 - val_loss: 1.9323 - val_RMSE: 1.8480\n",
      "Epoch 20/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.8231 - RMSE: 0.7403\n",
      "Epoch 20: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8232 - RMSE: 0.7374 - val_loss: 1.9429 - val_RMSE: 1.8596\n",
      "Epoch 21/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.8122 - RMSE: 0.7300\n",
      "Epoch 21: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.8124 - RMSE: 0.7281 - val_loss: 1.9703 - val_RMSE: 1.8867\n",
      "Epoch 22/25\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.7978 - RMSE: 0.7156\n",
      "Epoch 22: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.7987 - RMSE: 0.7165 - val_loss: 1.9536 - val_RMSE: 1.8701\n",
      "Epoch 23/25\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.7871 - RMSE: 0.7044\n",
      "Epoch 23: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.7879 - RMSE: 0.7082 - val_loss: 1.9513 - val_RMSE: 1.8670\n",
      "Epoch 24/25\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.7748 - RMSE: 0.6906\n",
      "Epoch 24: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.7757 - RMSE: 0.6908 - val_loss: 1.9750 - val_RMSE: 1.8892\n",
      "Epoch 25/25\n",
      "180/186 [============================>.] - ETA: 0s - loss: 0.7688 - RMSE: 0.6839\n",
      "Epoch 25: val_RMSE did not improve from 1.64594\n",
      "186/186 [==============================] - 2s 9ms/step - loss: 0.7684 - RMSE: 0.6847 - val_loss: 1.9738 - val_RMSE: 1.8873\n",
      "370/370 [==============================] - 1s 3ms/step\n",
      "평균 RMSE: 2.0458203258211363\n",
      "371/371 [==============================] - 1s 3ms/step\n",
      "1.200042822153313\n",
      "1.1422722406831898\n",
      "1.377355796818727\n",
      "Weights - 0.10 : 0.90 ; RMSE = 1.3341561\n",
      "Weights - 0.20 : 0.80 ; RMSE = 1.2944898\n",
      "Weights - 0.30 : 0.70 ; RMSE = 1.2586908\n",
      "Weights - 0.40 : 0.60 ; RMSE = 1.2270977\n",
      "Weights - 0.50 : 0.50 ; RMSE = 1.2000428\n",
      "Weights - 0.60 : 0.40 ; RMSE = 1.1778388\n",
      "Weights - 0.70 : 0.30 ; RMSE = 1.1607641\n",
      "Weights - 0.80 : 0.20 ; RMSE = 1.1490474\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1428535\n",
      "Weights - 0.88 : 0.12 ; RMSE = 1.1436450\n",
      "Weights - 0.89 : 0.11 ; RMSE = 1.1432212\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1428535\n",
      "Weights - 0.91 : 0.09 ; RMSE = 1.1425420\n",
      "Weights - 0.92 : 0.08 ; RMSE = 1.1422868\n",
      "Weights - 0.93 : 0.07 ; RMSE = 1.1420878\n",
      "Weights - 0.94 : 0.06 ; RMSE = 1.1419451\n",
      "Weights - 0.95 : 0.05 ; RMSE = 1.1418588\n",
      "Weights - 0.96 : 0.04 ; RMSE = 1.1418288\n",
      "Weights - 0.97 : 0.03 ; RMSE = 1.1418552\n",
      "Weights - 0.98 : 0.02 ; RMSE = 1.1419379\n",
      "Weights - 0.99 : 0.01 ; RMSE = 1.1420769\n",
      "371/371 [==============================] - 1s 3ms/step\n",
      "1.2043842828451492\n",
      "1.1493584726412156\n",
      "1.3730845996585894\n",
      "Weights - 0.10 : 0.90 ; RMSE = 1.3319058\n",
      "Weights - 0.20 : 0.80 ; RMSE = 1.2941395\n",
      "Weights - 0.30 : 0.70 ; RMSE = 1.2600925\n",
      "Weights - 0.40 : 0.60 ; RMSE = 1.2300737\n",
      "Weights - 0.50 : 0.50 ; RMSE = 1.2043843\n",
      "Weights - 0.60 : 0.40 ; RMSE = 1.1833064\n",
      "Weights - 0.70 : 0.30 ; RMSE = 1.1670897\n",
      "Weights - 0.80 : 0.20 ; RMSE = 1.1559391\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1500017\n",
      "Weights - 0.88 : 0.12 ; RMSE = 1.1507672\n",
      "Weights - 0.89 : 0.11 ; RMSE = 1.1503580\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1500017\n",
      "Weights - 0.91 : 0.09 ; RMSE = 1.1496984\n",
      "Weights - 0.92 : 0.08 ; RMSE = 1.1494482\n",
      "Weights - 0.93 : 0.07 ; RMSE = 1.1492510\n",
      "Weights - 0.94 : 0.06 ; RMSE = 1.1491070\n",
      "Weights - 0.95 : 0.05 ; RMSE = 1.1490161\n",
      "Weights - 0.96 : 0.04 ; RMSE = 1.1489783\n",
      "Weights - 0.97 : 0.03 ; RMSE = 1.1489936\n",
      "Weights - 0.98 : 0.02 ; RMSE = 1.1490621\n",
      "Weights - 0.99 : 0.01 ; RMSE = 1.1491837\n",
      "370/370 [==============================] - 1s 3ms/step\n",
      "1.211354904472811\n",
      "1.1520647665382375\n",
      "1.3870867928054038\n",
      "Weights - 0.10 : 0.90 ; RMSE = 1.3443823\n",
      "Weights - 0.20 : 0.80 ; RMSE = 1.3051352\n",
      "Weights - 0.30 : 0.70 ; RMSE = 1.2696662\n",
      "Weights - 0.40 : 0.60 ; RMSE = 1.2382998\n",
      "Weights - 0.50 : 0.50 ; RMSE = 1.2113549\n",
      "Weights - 0.60 : 0.40 ; RMSE = 1.1891321\n",
      "Weights - 0.70 : 0.30 ; RMSE = 1.1719000\n",
      "Weights - 0.80 : 0.20 ; RMSE = 1.1598810\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1532383\n",
      "Weights - 0.88 : 0.12 ; RMSE = 1.1541312\n",
      "Weights - 0.89 : 0.11 ; RMSE = 1.1536574\n",
      "Weights - 0.90 : 0.10 ; RMSE = 1.1532383\n",
      "Weights - 0.91 : 0.09 ; RMSE = 1.1528740\n",
      "Weights - 0.92 : 0.08 ; RMSE = 1.1525644\n",
      "Weights - 0.93 : 0.07 ; RMSE = 1.1523097\n",
      "Weights - 0.94 : 0.06 ; RMSE = 1.1521099\n",
      "Weights - 0.95 : 0.05 ; RMSE = 1.1519650\n",
      "Weights - 0.96 : 0.04 ; RMSE = 1.1518751\n",
      "Weights - 0.97 : 0.03 ; RMSE = 1.1518401\n",
      "Weights - 0.98 : 0.02 ; RMSE = 1.1518600\n",
      "Weights - 0.99 : 0.01 ; RMSE = 1.1519349\n"
     ]
    }
   ],
   "source": [
    "# Predictions using MF ###########################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path='/home/recordk/reco/BX-Book-Ratings.csv'\n",
    "ratings = pd.read_csv(path)\n",
    "ratings['Book-Rating'] = ratings['Book-Rating'].astype(int)\n",
    "ratings.columns=['user_id','isbn','rating']\n",
    "\n",
    "# 0점은 다 제거\n",
    "ratings=ratings[ratings['rating']!=0]\n",
    "ratings=ratings.reset_index(drop=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#  user과 isbn의 인덱스를 정해주기 위해 LabelEncoder 사용! \n",
    "user_encoder = LabelEncoder()\n",
    "isbn_encoder = LabelEncoder()\n",
    "\n",
    "# user_id와 isbn label인코딩\n",
    "ratings['user_id'] = user_encoder.fit_transform(ratings['user_id'])\n",
    "ratings['isbn'] = isbn_encoder.fit_transform(ratings['isbn'])\n",
    "\n",
    "\n",
    "\n",
    "# mf class\n",
    "class NEW_MF():\n",
    "    # user - book matrix 행렬을 받음\n",
    "    def __init__(self, ratings, K, alpha, beta, iterations, tolerance=0.005, verbose=True):\n",
    "        self.R = np.array(ratings)\n",
    "        # book id와 index 리스트 선언\n",
    "        item_id_index = []\n",
    "        # index와 book id 리스트 선언\n",
    "        index_item_id = []\n",
    "        \n",
    "        for i, one_id in enumerate(ratings):\n",
    "            item_id_index.append([one_id, i])\n",
    "            index_item_id.append([i, one_id])\n",
    "            \n",
    "        # 딕셔너리화\n",
    "        self.item_id_index = dict(item_id_index)\n",
    "        self.index_item_id = dict(index_item_id)\n",
    "        \n",
    "        # user와 index 리스트 선언        \n",
    "        user_id_index = []\n",
    "        # index와 user와 리스트 선언\n",
    "        index_user_id = []\n",
    "        \n",
    "        # book user matrix 행렬을 받음\n",
    "        for i, one_id in enumerate(ratings.T):\n",
    "            user_id_index.append([one_id, i])\n",
    "            index_user_id.append([i, one_id])\n",
    "            \n",
    "        # 딕셔너리화\n",
    "        self.user_id_index = dict(user_id_index)\n",
    "        self.index_user_id = dict(index_user_id)\n",
    "        \n",
    "        # 파라미터 선언\n",
    "        self.num_users, self.num_items = np.shape(self.R)\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        # print(len(self.user_id_index))\n",
    "        \n",
    "    # 테스트 데이터 세팅\n",
    "    def set_test(self, ratings_test):                           # Setting test set\n",
    "        test_set = []\n",
    "        for i in range(len(ratings_test)):                      # Selected ratings\n",
    "            # print(self.user_id_index)\n",
    "            # print(self.user_id_index[4738])\n",
    "            \n",
    "            # 인덱스 확인해서 유저의 인덱스에 해당하는 것\n",
    "            x = self.user_id_index[ratings_test.iloc[i,0]]\n",
    "            \n",
    "            # 인덱스 확인해서 book의 인덱스에 해당하는 것\n",
    "            y = self.item_id_index[ratings_test.iloc[i,1]]\n",
    "            \n",
    "            # 점수\n",
    "            z = ratings_test.iloc[i,2]\n",
    "            \n",
    "            # 테스트 데이터 \n",
    "            test_set.append([x, y, z])\n",
    "            \n",
    "            # 테스트 데이터에 해당되는 원본데이터 점수 0점으로 만듬\n",
    "            self.R[x, y] = 0                    \n",
    "        self.test_set = test_set\n",
    "        return test_set                         \n",
    "\n",
    "    def test(self):                             # Training 하면서 test set의 정확도를 계산하는 메소드 \n",
    "        # P,Q 선언\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        # bias 선언\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[self.R.nonzero()])\n",
    "\n",
    "        # 비어있는값 삭제 -> bookratings는 이미 비어있는 값이 없으므로 사실상 필요 x\n",
    "        rows, columns = self.R.nonzero()\n",
    "        \n",
    "        # 샘플값\n",
    "        self.samples = [(i,j, self.R[i,j]) for i, j in zip(rows, columns)]\n",
    "\n",
    "        \n",
    "        # for문을 통해서 비교 후 rmse가 작은 값을 찾기 위해 큰값을 best RMSE에 선언\n",
    "        best_RMSE = 10000\n",
    "        \n",
    "        # iteration확인하기 위한 선언\n",
    "        best_iteration = 0\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)#샘플 셔플\n",
    "            self.sgd()# sgd\n",
    "            rmse1 = self.rmse()\n",
    "            rmse2 = self.test_rmse() # test rmse\n",
    "            training_process.append((i, rmse1, rmse2)) # 진행과정 확인위한 리스트\n",
    "            \n",
    "            # 출력되는부분\n",
    "            if self.verbose:# default값이 True설정 False하면 안보임\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1, rmse1, rmse2))\n",
    "            \n",
    "            # rmse값이 더 좋으면 bestrmse에 반영\n",
    "            if best_RMSE > rmse2:                      # New best record\n",
    "                best_RMSE = rmse2\n",
    "                best_iteration = i\n",
    "            elif (rmse2 - best_RMSE) > self.tolerance: # 점수가 더 나아지지 않으면 break\n",
    "                break\n",
    "        print(best_iteration, best_RMSE) # 출력하는부분\n",
    "        return training_process,best_RMSE # 평균값을 받기 위해 리턴값 하나 더 추가\n",
    "\n",
    "    # sgd 함수\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples: #샘플을 받아서 \n",
    "            prediction = self.get_prediction(i, j) # 예측\n",
    "            error = (r - prediction) #loss 평가\n",
    "            self.b_u[i] += self.alpha * (error - self.beta * self.b_u[i]) # 파라미터값과 계산하여 가중치 변화\n",
    "            self.b_d[j] += self.alpha * (error - self.beta * self.b_d[j]) # ==\n",
    "\n",
    "            self.Q[j, :] += self.alpha * (error * self.P[i, :] - self.beta * self.Q[j,:]) # ==\n",
    "            self.P[i, :] += self.alpha * (error * self.Q[j, :] - self.beta * self.P[i,:]) # ==\n",
    "\n",
    "    # rmse 측정 학습 rmse를 평가하는 함수\n",
    "    def rmse(self):\n",
    "        rows, columns = self.R.nonzero()\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        for x, y in zip(rows, columns):\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "\n",
    "    # Test RMSE 계산하는 method \n",
    "    def test_rmse(self):\n",
    "        error = 0\n",
    "        for one_set in self.test_set:\n",
    "            predicted = self.get_prediction(one_set[0], one_set[1])\n",
    "            error += pow(one_set[2] - predicted, 2)\n",
    "        return np.sqrt(error/len(self.test_set))\n",
    "\n",
    "    # predict하는 함수\n",
    "    def get_prediction(self, i, j):\n",
    "        prediction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    # predict하는 함수\n",
    "    def get_one_prediction(self, user_id, isbn):\n",
    "        return self.get_prediction(self.user_id_index[user_id], self.item_id_index[isbn])\n",
    "\n",
    "\n",
    "# kfold이용해서 트레인 테스트 3개로 분리\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# rmse리스트\n",
    "average_rmse_values = []\n",
    "\n",
    "# train index test index 무작위로 1/3 나눔\n",
    "for train_index, test_index in kf.split(ratings):\n",
    "    \n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "    \n",
    "    # Train + test index의 리스트를 각각받아 원본데이터를 나눔\n",
    "    train_data = ratings.iloc[train_index]\n",
    "    test_data = ratings.iloc[test_index]\n",
    "    # print(test_data)\n",
    "    \n",
    "    # 피봇으로 만듬 fillna는 사실상 안쓰임 + user_id isbn 행렬을 만듬\n",
    "    temp = ratings.pivot(index = 'user_id', columns ='isbn', values = 'rating').fillna(0)\n",
    "    # mf = NEW_MF(temp, K=220, alpha=0.0014, beta=0.075, iterations=350, tolerance=0.0001, verbose=True)\n",
    "    \n",
    "    # 인스턴스생성 + 파라미터를 대입\n",
    "    mf = NEW_MF(temp, K=30, alpha=0.01, beta=0.02, iterations=100, tolerance=0.01, verbose=True)\n",
    "    \n",
    "    # Test 데이터 세팅\n",
    "    test_set = mf.set_test(test_data)\n",
    "    \n",
    "    # 추가 리턴을 받아서 rmse또한 반환\n",
    "    result,rmse = mf.test()\n",
    "    \n",
    "    # 평균을 계산하기 위한 리스트\n",
    "    average_rmse_values.append(rmse)\n",
    "\n",
    "# 전체 폴드에 대한 RMSE 평균 계산 및 출력\n",
    "average_rmse = np.mean(average_rmse_values)\n",
    "\n",
    "#평균 RMSE\n",
    "print(\"평균 RMSE:\", average_rmse)\n",
    "\n",
    "\n",
    "# Predictions using DL ###########################################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.layers import Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "# Variable 초기화 \n",
    "K = 350                         # Latent factor 수 \n",
    "lr = 0.01                           # 학습률\n",
    "reg = 0.005                     # Regularization penalty\n",
    "\n",
    "#tf RMSE계산\n",
    "def RMSE(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "# 모델 구조\n",
    "def architecture(train_data, test_data,lr):\n",
    "    # Keras model\n",
    "    user = Input(shape=(1,))  # User input\n",
    "    item = Input(shape=(1,))  # Item input\n",
    "    P_embedding = Embedding(N, K, embeddings_regularizer=l2(reg))(user)  # (N, 1, K)\n",
    "    Q_embedding = Embedding(M, K, embeddings_regularizer=l2(reg))(item)  # (M, 1, K)\n",
    "    user_bias = Embedding(N, 1, embeddings_regularizer=l2(reg))(user)  # User bias term (N, 1, )\n",
    "    item_bias = Embedding(M, 1, embeddings_regularizer=l2(reg))(item)  # Item bias term (M, 1, )\n",
    "\n",
    "    # P,Q임베딩 및 bias 연결\n",
    "    P_embedding = Flatten()(P_embedding)  # (K, )\n",
    "    Q_embedding = Flatten()(Q_embedding)  # (K, )\n",
    "    user_bias = Flatten()(user_bias)  # (1, )\n",
    "    item_bias = Flatten()(item_bias)  # (1, )\n",
    "    R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias])  # (2K + 2, )\n",
    "\n",
    "    # 신경망 층 구성! swish 활성화함수 활용하여 3개층구성\n",
    "    R = Dense(256)(R)\n",
    "    R = Activation('swish')(R)\n",
    "    R = Dense(128)(R)\n",
    "    R = Activation('swish')(R)\n",
    "    R = Dense(64)(R)\n",
    "    R = Activation('swish')(R)\n",
    "    R = Dense(1)(R)\n",
    "\n",
    "    # 모델 선언\n",
    "    model = Model(inputs=[user, item], outputs=R)\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        loss=RMSE,\n",
    "        #optimizer=SGD(lr=0.1, momentum=0.9),\n",
    "        # adamax optimizer\n",
    "        optimizer=Adamax(lr=lr),\n",
    "        metrics=[RMSE]\n",
    "    )\n",
    "\n",
    "    #checkpoint경로 설정\n",
    "    checkpoint_path = 'CheckPoint'\n",
    "    \n",
    "    #모델 체크포인트\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True,\n",
    "                                monitor='val_RMSE',\n",
    "                                verbose=1)\n",
    "    # 모델 학습 및 결과 도출\n",
    "    result = model.fit(\n",
    "        x=[train_data.user_id.values, train_data.isbn.values],\n",
    "        y=train_data.rating.values - mu, # ratings에 평균을 빼서 정규화\n",
    "        callbacks=[checkpoint], # 콜백함수 선언 tensorboard나 earlystopping등 많은 콜백함수 사용가능\n",
    "        epochs=25,\n",
    "        batch_size=128,\n",
    "        validation_data=(\n",
    "            [test_data.user_id.values, test_data.isbn.values],\n",
    "            test_data.rating.values - mu\n",
    "        )\n",
    "    )\n",
    "    # 모델 가중치 불러오기\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    # 모델과 결과 반환\n",
    "    return model, result\n",
    "\n",
    "# 사용자 수 및 도서 수\n",
    "N = len(set(ratings.user_id)) + 1\n",
    "M = len(set(ratings.isbn)) + 1\n",
    "\n",
    "\n",
    "# kfold이용해서 트레인 테스트 3개로 분리\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# rmse리스트\n",
    "rmse_values = []\n",
    "\n",
    "# for문\n",
    "for train_indices, test_indices in kf.split(ratings):\n",
    "    train_data = ratings.iloc[train_indices].reset_index(drop=True)\n",
    "    test_data = ratings.iloc[test_indices].reset_index(drop=True)\n",
    "    \n",
    "    mu = train_data.rating.mean()    # 전체 평균 \n",
    "\n",
    "    # 셔플\n",
    "    train_data = shuffle(train_data)\n",
    "\n",
    "    # 모델 및 결과\n",
    "    model, result = architecture(train_data, test_data,lr)\n",
    "\n",
    "    # 모델 평가\n",
    "    predictions = model.predict([test_data.user_id.values, test_data.isbn.values]) + mu\n",
    "    rmse = RMSE(test_data.rating.values, predictions)\n",
    "    rmse_values.append(rmse)\n",
    "\n",
    "# rmse 평균\n",
    "average_rmse = np.mean(rmse_values)\n",
    "print(\"평균 RMSE:\", average_rmse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hybrid recommendation ###########################################################################\n",
    "\n",
    "# MF추천\n",
    "def recommender0(recomm_list, mf):\n",
    "    id_pairs = zip(recomm_list[:, 0], recomm_list[:, 1])\n",
    "    recommendations = np.array([mf.get_one_prediction(user, isbn) for (user, isbn) in id_pairs])\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# 딥러닝 모델 추천\n",
    "def recommender1(recomm_list, model):\n",
    "    user_ids = recomm_list[:, 0]\n",
    "    isbn = recomm_list[:, 1]\n",
    "    recommendations = model.predict([user_ids, isbn]) + mu\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# kfold이용해서 train test 3개로 분리\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# rmse리스트\n",
    "rmse_values = []\n",
    "\n",
    "# for문\n",
    "for train_indices, test_indices in kf.split(ratings):\n",
    "    train_data = ratings.iloc[train_indices].reset_index(drop=True)\n",
    "    test_data = ratings.iloc[test_indices].reset_index(drop=True)\n",
    "    \n",
    "    # test 데이터 사용하여 평가\n",
    "    recomm_list = np.array(test_data.iloc[:, [0, 1]])\n",
    "    \n",
    "    # mf결과\n",
    "    result0 = recommender0(recomm_list, mf)\n",
    "    \n",
    "    # 딥러닝 결과\n",
    "    result1 = np.ravel(recommender1(recomm_list, model))\n",
    "\n",
    "    # 각각의 모델에 대한 가중치 선언\n",
    "    weight = [0.5, 0.5]\n",
    "    predictions = []\n",
    "    for i, number in enumerate(result0):\n",
    "        predictions.append(result0[i] * weight[0] + result1[i] * weight[1])\n",
    "        \n",
    "    # 각각의 모델에 대한 rmse 평가\n",
    "    print(RMSE2(test_data['rating'], predictions))\n",
    "    print(RMSE2(test_data['rating'], result0))\n",
    "    print(RMSE2(test_data['rating'], result1))\n",
    "\n",
    "    # 가중치 조절\n",
    "    for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        weight = [i, 1 - i]\n",
    "        predictions = []\n",
    "        for i, number in enumerate(result0):\n",
    "            predictions.append(result0[i] * weight[0] + result1[i] * weight[1])\n",
    "        print(\"Weights - %.2f : %.2f ; RMSE = %.7f\" % (weight[0], weight[1], RMSE2(test_data['rating'], predictions)))\n",
    "\n",
    "    # 가중치 조절 2\n",
    "    for i in [0.88, 0.89, 0.90, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]:\n",
    "        weight = [i, 1 - i]\n",
    "        predictions = []\n",
    "        for i, number in enumerate(result0):\n",
    "            predictions.append(result0[i] * weight[0] + result1[i] * weight[1])\n",
    "        print(\"Weights - %.2f : %.2f ; RMSE = %.7f\" % (weight[0], weight[1], RMSE2(test_data['rating'], predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 최종\n",
    "Weights - 0.96 : 0.04 ; RMSE = 1.1489783\n",
    "Weights - 0.97 : 0.03 ; RMSE = 1.1518401\n",
    "Weights - 0.96 : 0.04 ; RMSE = 1.1418288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1475490666666666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE1 = 1.1489783\n",
    "RMSE2= 1.1518401\n",
    "RMSE3= 1.1418288\n",
    "np.mean([RMSE1,RMSE2,RMSE3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "re",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
